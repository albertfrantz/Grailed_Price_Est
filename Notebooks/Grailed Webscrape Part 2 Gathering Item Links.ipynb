{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grailed Webscrape Part 2: Collecting Item Links\n",
    "\n",
    "This is the second Grailed webscraping notebook. In those notebook we will take the designers sold links gathered in the previous notebook and gather individual item links from each. Due to time constraints I decided to not gather from designers that had less than 25 items sold and would gather a maximum of 4 page scrolls worth from designers with many items sold. I also ran this on multiple computers at the same time to speed up the proccess so that is why some cells have not been run here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.common.exceptions import NoSuchElementException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading in all 6 sold link files\n",
    "sold_links_1 = pd.read_csv('../Data/Designer_Sold_Page_Links/sold_links_1000.csv')\n",
    "sold_links_3 = pd.read_csv('../Data/Designer_Sold_Page_Links/sold_links_3000.csv')\n",
    "sold_links_4 = pd.read_csv('../Data/Designer_Sold_Page_Links/sold_links_4000.csv')\n",
    "sold_links_5 = pd.read_csv('../Data/Designer_Sold_Page_Links/sold_links_5000_3.csv')\n",
    "sold_links_6 = pd.read_csv('../Data/Designer_Sold_Page_Links/sold_links_6000.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Once again because of how long this process will take I broke this process into 6 parts incase my computer crashed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "Timed out waiting for page to load\n",
      "Timed out waiting for page to load\n",
      "Timed out waiting for page to load\n",
      "70\n",
      "80\n",
      "90\n",
      "Timed out waiting for page to load\n",
      "Timed out waiting for page to load\n",
      "100\n",
      "110\n",
      "120\n",
      "Timed out waiting for page to load\n",
      "130\n",
      "Timed out waiting for page to load\n",
      "Timed out waiting for page to load\n",
      "Timed out waiting for page to load\n",
      "Timed out waiting for page to load\n",
      "Timed out waiting for page to load\n",
      "Timed out waiting for page to load\n",
      "140\n",
      "150\n",
      "Timed out waiting for page to load\n",
      "160\n",
      "Timed out waiting for page to load\n",
      "Timed out waiting for page to load\n",
      "170\n",
      "180\n",
      "Timed out waiting for page to load\n",
      "190\n",
      "Timed out waiting for page to load\n",
      "200\n",
      "Timed out waiting for page to load\n",
      "Timed out waiting for page to load\n",
      "210\n",
      "220\n",
      "230\n",
      "Timed out waiting for page to load\n",
      "Timed out waiting for page to load\n",
      "Timed out waiting for page to load\n",
      "Timed out waiting for page to load\n",
      "240\n",
      "Timed out waiting for page to load\n",
      "250\n",
      "260\n",
      "270\n",
      "Timed out waiting for page to load\n",
      "Timed out waiting for page to load\n",
      "Timed out waiting for page to load\n",
      "280\n",
      "Timed out waiting for page to load\n",
      "290\n",
      "300\n",
      "Timed out waiting for page to load\n",
      "Timed out waiting for page to load\n",
      "Timed out waiting for page to load\n",
      "310\n",
      "320\n",
      "Timed out waiting for page to load\n",
      "Timed out waiting for page to load\n",
      "Timed out waiting for page to load\n",
      "330\n",
      "340\n",
      "350\n",
      "Timed out waiting for page to load\n",
      "Timed out waiting for page to load\n",
      "360\n",
      "Timed out waiting for page to load\n",
      "Timed out waiting for page to load\n",
      "370\n",
      "380\n",
      "Timed out waiting for page to load\n",
      "390\n",
      "Timed out waiting for page to load\n",
      "400\n",
      "Timed out waiting for page to load\n",
      "Timed out waiting for page to load\n",
      "410\n",
      "Timed out waiting for page to load\n",
      "420\n",
      "Timed out waiting for page to load\n",
      "430\n",
      "Timed out waiting for page to load\n",
      "440\n",
      "450\n",
      "Timed out waiting for page to load\n",
      "460\n",
      "Timed out waiting for page to load\n",
      "470\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "Link = [] # list for links of items to be added\n",
    "\n",
    "# going through first 1,000 designer's sold links\n",
    "for link in sold_links_1['Link']:\n",
    "    try:\n",
    "        # Set a URL\n",
    "        base_url = link\n",
    "\n",
    "        # open up chrome.\n",
    "        chrome_options = webdriver.ChromeOptions()\n",
    "        chrome_options.add_argument(\"--start-maximized\")\n",
    "        driver = webdriver.Chrome(\"C:/Users/alber/Documents/chromedriver.exe\",options=chrome_options) # replace location with your webdriver location\n",
    "        driver.get(base_url)\n",
    "\n",
    "        # wait 30 sec\n",
    "        timeout = 20\n",
    "        \n",
    "    except Exception as e:\n",
    "        driver.quit()\n",
    "        print(e)\n",
    "        continue     \n",
    "    \n",
    "    try:\n",
    "        WebDriverWait(driver, timeout).until(EC.visibility_of_element_located((By.XPATH, \"//div[@class='feed-item']\")))\n",
    "    except TimeoutException:\n",
    "        print(\"Timed out waiting for page to load\")\n",
    "        driver.quit()\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        ScrollCount = 0\n",
    "        # getting the number of items on the page. if it is a full page then it will be 40\n",
    "        results = driver.find_elements_by_xpath('//div[@class=\"FiltersInstantSearch\"]//div[@class=\"feed-item\"]') \n",
    "    except Exception as e:\n",
    "        driver.quit()\n",
    "        print(e)\n",
    "        continue\n",
    "        \n",
    "    # if the number of items is less than 25 sold we will not be collecting that data   \n",
    "    if len(results) < 25:\n",
    "        driver.quit()\n",
    "        continue\n",
    "    \n",
    "    # while page is full will continue to scroll up to a maximum of 4 scrolls\n",
    "    try:\n",
    "        while (len(results) % 40 == 0) and (ScrollCount < 4):\n",
    "            driver.execute_script(\"window.scrollTo(0,document.body.scrollHeight)\")\n",
    "            ScrollCount += 1\n",
    "            results = driver.find_elements_by_xpath('//div[@class=\"FiltersInstantSearch\"]//div[@class=\"feed-item\"]')\n",
    "            time.sleep(1)\n",
    "    except Exception as e:\n",
    "        driver.quit()\n",
    "        print(e)\n",
    "        continue\n",
    "    \n",
    "    # for each item link in results will get the link and append to link list\n",
    "    try:\n",
    "        for result in results:\n",
    "                link = result.find_element_by_xpath('./a').get_attribute(\"href\")\n",
    "                Link.append(link)\n",
    "\n",
    "    except Exception as e:\n",
    "        driver.quit()\n",
    "        print(e)\n",
    "        continue\n",
    "    \n",
    "    \n",
    "    driver.quit()\n",
    "    \n",
    "    count += 1\n",
    "    \n",
    "    if count%10 == 0:\n",
    "        print(count)\n",
    "    \n",
    "        \n",
    "# Turn the lists into a DataFrame and saves\n",
    "ItemDF_1=pd.DataFrame(Link,columns=['Link'])\n",
    "ItemDF_1.to_csv('Sold_Items_1.csv')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## repeat the same process for the other 5 designer sold links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "20\n",
      "Timed out waiting for page to load\n",
      "Timed out waiting for page to load\n",
      "30\n",
      "40\n",
      "Timed out waiting for page to load\n",
      "50\n",
      "Timed out waiting for page to load\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "Timed out waiting for page to load\n",
      "120\n",
      "130\n",
      "Timed out waiting for page to load\n",
      "140\n",
      "150\n",
      "Timed out waiting for page to load\n",
      "Timed out waiting for page to load\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "Timed out waiting for page to load\n",
      "Timed out waiting for page to load\n",
      "Timed out waiting for page to load\n",
      "200\n",
      "Timed out waiting for page to load\n",
      "Timed out waiting for page to load\n",
      "210\n",
      "220\n",
      "Timed out waiting for page to load\n",
      "Timed out waiting for page to load\n",
      "230\n",
      "Timed out waiting for page to load\n",
      "Timed out waiting for page to load\n",
      "Timed out waiting for page to load\n",
      "240\n",
      "250\n",
      "Timed out waiting for page to load\n",
      "260\n",
      "Timed out waiting for page to load\n",
      "270\n",
      "Timed out waiting for page to load\n",
      "280\n",
      "290\n",
      "Timed out waiting for page to load\n",
      "Timed out waiting for page to load\n",
      "300\n",
      "310\n",
      "Timed out waiting for page to load\n",
      "Timed out waiting for page to load\n",
      "320\n",
      "330\n",
      "340\n",
      "350\n",
      "Timed out waiting for page to load\n",
      "360\n",
      "370\n",
      "Timed out waiting for page to load\n",
      "380\n",
      "Timed out waiting for page to load\n",
      "390\n",
      "Timed out waiting for page to load\n",
      "Timed out waiting for page to load\n",
      "400\n",
      "Timed out waiting for page to load\n",
      "410\n",
      "Timed out waiting for page to load\n",
      "420\n",
      "430\n",
      "Timed out waiting for page to load\n",
      "440\n",
      "450\n",
      "460\n",
      "Timed out waiting for page to load\n",
      "470\n",
      "Timed out waiting for page to load\n",
      "480\n",
      "490\n",
      "500\n",
      "510\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "Link =[]\n",
    "for link in sold_links_2['Link']:\n",
    "    try:\n",
    "        # Set a URL\n",
    "        base_url = link\n",
    "\n",
    "        # open up chrome. \n",
    "        chrome_options = webdriver.ChromeOptions()\n",
    "        chrome_options.add_argument(\"--start-maximized\")\n",
    "        driver = webdriver.Chrome(\"C:/Users/alber/Documents/chromedriver.exe\",options=chrome_options)\n",
    "        driver.get(base_url)\n",
    "\n",
    "        # wait 30 sec\n",
    "        timeout = 20\n",
    "        \n",
    "    except Exception as e:\n",
    "        driver.quit()\n",
    "        print(e)\n",
    "        continue     \n",
    "    \n",
    "    try:\n",
    "        WebDriverWait(driver, timeout).until(EC.visibility_of_element_located((By.XPATH, \"//div[@class='feed-item']\")))\n",
    "    except TimeoutException:\n",
    "        print(\"Timed out waiting for page to load\")\n",
    "        driver.quit()\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        ScrollCount = 0\n",
    "        results = driver.find_elements_by_xpath('//div[@class=\"FiltersInstantSearch\"]//div[@class=\"feed-item\"]')\n",
    "    except Exception as e:\n",
    "        driver.quit()\n",
    "        print(e)\n",
    "        continue\n",
    "           \n",
    "    if len(results) < 25:\n",
    "        driver.quit()\n",
    "        continue\n",
    "        \n",
    "    try:\n",
    "        while (len(results) % 40 == 0) and (ScrollCount < 4):\n",
    "            driver.execute_script(\"window.scrollTo(0,document.body.scrollHeight)\")\n",
    "            ScrollCount += 1\n",
    "            results = driver.find_elements_by_xpath('//div[@class=\"FiltersInstantSearch\"]//div[@class=\"feed-item\"]')\n",
    "            time.sleep(1)\n",
    "    except Exception as e:\n",
    "        driver.quit()\n",
    "        print(e)\n",
    "        continue\n",
    "     \n",
    "    try:\n",
    "        for result in results:\n",
    "                link = result.find_element_by_xpath('./a').get_attribute(\"href\")\n",
    "                Link.append(link)\n",
    "\n",
    "    except Exception as e:\n",
    "        driver.quit()\n",
    "        print(e)\n",
    "        continue\n",
    "    \n",
    "    \n",
    "    driver.quit()\n",
    "    \n",
    "    count += 1\n",
    "    \n",
    "    if count%10 == 0:\n",
    "        print(count)\n",
    "    \n",
    "        \n",
    "# Turn the lists into a DataFrame and saved\n",
    "ItemDF_2=pd.DataFrame(Link,columns=['Link'])\n",
    "ItemDF_2.to_csv('Sold_Items_2.csv')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timed out waiting for page to load\n",
      "Timed out waiting for page to load\n",
      "Timed out waiting for page to load\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "Timed out waiting for page to load\n",
      "Timed out waiting for page to load\n",
      "50\n",
      "60\n",
      "Timed out waiting for page to load\n",
      "70\n",
      "Timed out waiting for page to load\n",
      "Timed out waiting for page to load\n",
      "Timed out waiting for page to load\n",
      "80\n",
      "Timed out waiting for page to load\n",
      "Timed out waiting for page to load\n",
      "Timed out waiting for page to load\n",
      "Timed out waiting for page to load\n",
      "Timed out waiting for page to load\n",
      "90\n",
      "Timed out waiting for page to load\n",
      "Timed out waiting for page to load\n",
      "100\n",
      "Timed out waiting for page to load\n",
      "Timed out waiting for page to load\n",
      "110\n",
      "120\n",
      "Timed out waiting for page to load\n",
      "130\n",
      "Timed out waiting for page to load\n",
      "140\n",
      "Timed out waiting for page to load\n",
      "150\n",
      "Timed out waiting for page to load\n",
      "160\n",
      "Timed out waiting for page to load\n",
      "Timed out waiting for page to load\n",
      "170\n",
      "180\n",
      "190\n",
      "Timed out waiting for page to load\n",
      "200\n",
      "Timed out waiting for page to load\n",
      "Timed out waiting for page to load\n",
      "210\n",
      "Timed out waiting for page to load\n",
      "220\n",
      "Timed out waiting for page to load\n",
      "230\n",
      "Timed out waiting for page to load\n",
      "240\n",
      "Timed out waiting for page to load\n",
      "250\n",
      "Timed out waiting for page to load\n",
      "Timed out waiting for page to load\n",
      "260\n",
      "270\n",
      "Timed out waiting for page to load\n",
      "280\n",
      "290\n",
      "Timed out waiting for page to load\n",
      "300\n",
      "310\n",
      "Timed out waiting for page to load\n",
      "320\n",
      "330\n",
      "Timed out waiting for page to load\n",
      "340\n",
      "350\n",
      "Timed out waiting for page to load\n",
      "Timed out waiting for page to load\n",
      "360\n",
      "Timed out waiting for page to load\n",
      "370\n",
      "Timed out waiting for page to load\n",
      "380\n",
      "Timed out waiting for page to load\n",
      "Timed out waiting for page to load\n",
      "390\n",
      "Timed out waiting for page to load\n",
      "400\n",
      "410\n",
      "Timed out waiting for page to load\n",
      "Timed out waiting for page to load\n",
      "Timed out waiting for page to load\n",
      "420\n",
      "Timed out waiting for page to load\n",
      "Timed out waiting for page to load\n",
      "430\n",
      "Timed out waiting for page to load\n",
      "440\n",
      "Timed out waiting for page to load\n",
      "450\n",
      "460\n",
      "Timed out waiting for page to load\n",
      "470\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "Link =[]\n",
    "for link in sold_links_3['Link']:\n",
    "    try:\n",
    "        # Set a URL\n",
    "        base_url = link\n",
    "\n",
    "        # open up chrome.\n",
    "        chrome_options = webdriver.ChromeOptions()\n",
    "        chrome_options.add_argument(\"--start-maximized\")\n",
    "        driver = webdriver.Chrome(\"C:/Users/alber/Documents/chromedriver.exe\",options=chrome_options)\n",
    "        driver.get(base_url)\n",
    "\n",
    "        # wait 30 sec\n",
    "        timeout = 20\n",
    "        \n",
    "    except Exception as e:\n",
    "        driver.quit()\n",
    "        print(e)\n",
    "        continue     \n",
    "    \n",
    "    try:\n",
    "        WebDriverWait(driver, timeout).until(EC.visibility_of_element_located((By.XPATH, \"//div[@class='feed-item']\")))\n",
    "    except TimeoutException:\n",
    "        print(\"Timed out waiting for page to load\")\n",
    "        driver.quit()\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        ScrollCount = 0\n",
    "        results = driver.find_elements_by_xpath('//div[@class=\"FiltersInstantSearch\"]//div[@class=\"feed-item\"]')\n",
    "    except Exception as e:\n",
    "        driver.quit()\n",
    "        print(e)\n",
    "        continue\n",
    "           \n",
    "    if len(results) < 25:\n",
    "        driver.quit()\n",
    "        continue\n",
    "        \n",
    "    try:\n",
    "        while (len(results) % 40 == 0) and (ScrollCount < 4):\n",
    "            driver.execute_script(\"window.scrollTo(0,document.body.scrollHeight)\")\n",
    "            ScrollCount += 1\n",
    "            results = driver.find_elements_by_xpath('//div[@class=\"FiltersInstantSearch\"]//div[@class=\"feed-item\"]')\n",
    "            time.sleep(1)\n",
    "    except Exception as e:\n",
    "        driver.quit()\n",
    "        print(e)\n",
    "        continue\n",
    "     \n",
    "    try:\n",
    "        for result in results:\n",
    "                link = result.find_element_by_xpath('./a').get_attribute(\"href\")\n",
    "                Link.append(link)\n",
    "\n",
    "    except Exception as e:\n",
    "        driver.quit()\n",
    "        print(e)\n",
    "        continue\n",
    "    \n",
    "    \n",
    "    driver.quit()\n",
    "    \n",
    "    count += 1\n",
    "    \n",
    "    if count%10 == 0:\n",
    "        print(count)\n",
    "    \n",
    "        \n",
    "# Turn the lists into a DataFrame\n",
    "ItemDF_3=pd.DataFrame(Link,columns=['Link'])\n",
    "ItemDF_3.to_csv('Sold_Items_3.csv')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "Link =[]\n",
    "for link in sold_links_4['Link']:\n",
    "    try:\n",
    "        # Set a URL\n",
    "        base_url = link\n",
    "\n",
    "        # open up chrome.\n",
    "        chrome_options = webdriver.ChromeOptions()\n",
    "        chrome_options.add_argument(\"--start-maximized\")\n",
    "        driver = webdriver.Chrome(\"C:/Users/alber/Documents/chromedriver.exe\",options=chrome_options)\n",
    "        driver.get(base_url)\n",
    "\n",
    "        # wait 30 sec\n",
    "        timeout = 20\n",
    "        \n",
    "    except Exception as e:\n",
    "        driver.quit()\n",
    "        print(e)\n",
    "        continue     \n",
    "    \n",
    "    try:\n",
    "        WebDriverWait(driver, timeout).until(EC.visibility_of_element_located((By.XPATH, \"//div[@class='feed-item']\")))\n",
    "    except TimeoutException:\n",
    "        print(\"Timed out waiting for page to load\")\n",
    "        driver.quit()\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        ScrollCount = 0\n",
    "        results = driver.find_elements_by_xpath('//div[@class=\"FiltersInstantSearch\"]//div[@class=\"feed-item\"]')\n",
    "    except Exception as e:\n",
    "        driver.quit()\n",
    "        print(e)\n",
    "        continue\n",
    "        \n",
    "    if len(results) < 25:\n",
    "        driver.quit()\n",
    "        continue\n",
    "        \n",
    "    try:\n",
    "        while (len(results) % 40 == 0) and (ScrollCount < 4):\n",
    "            driver.execute_script(\"window.scrollTo(0,document.body.scrollHeight)\")\n",
    "            ScrollCount += 1\n",
    "            results = driver.find_elements_by_xpath('//div[@class=\"FiltersInstantSearch\"]//div[@class=\"feed-item\"]')\n",
    "            time.sleep(1)\n",
    "    except Exception as e:\n",
    "        driver.quit()\n",
    "        print(e)\n",
    "        continue\n",
    "     \n",
    "    try:\n",
    "        for result in results:\n",
    "                link = result.find_element_by_xpath('./a').get_attribute(\"href\")\n",
    "                Link.append(link)\n",
    "\n",
    "    except Exception as e:\n",
    "        driver.quit()\n",
    "        print(e)\n",
    "        continue\n",
    "    \n",
    "    \n",
    "    driver.quit()\n",
    "    \n",
    "    count += 1\n",
    "    \n",
    "    if count%10 == 0:\n",
    "        print(count)\n",
    "    \n",
    "        \n",
    "# Turn the lists into a DataFrame\n",
    "ItemDF_4=pd.DataFrame(Link,columns=['Link'])\n",
    "ItemDF_4.to_csv('Sold_Items_4.csv')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "Link =[]\n",
    "for link in sold_links_5['Link']:\n",
    "    try:\n",
    "        # Set a URL\n",
    "        base_url = link\n",
    "\n",
    "        # open up chrome.\n",
    "        chrome_options = webdriver.ChromeOptions()\n",
    "        chrome_options.add_argument(\"--start-maximized\")\n",
    "        driver = webdriver.Chrome(\"C:/Users/alber/Documents/chromedriver.exe\",options=chrome_options)\n",
    "        driver.get(base_url)\n",
    "\n",
    "        # wait 30 sec\n",
    "        timeout = 20\n",
    "        \n",
    "    except Exception as e:\n",
    "        driver.quit()\n",
    "        print(e)\n",
    "        continue     \n",
    "    \n",
    "    try:\n",
    "        WebDriverWait(driver, timeout).until(EC.visibility_of_element_located((By.XPATH, \"//div[@class='feed-item']\")))\n",
    "    except TimeoutException:\n",
    "        print(\"Timed out waiting for page to load\")\n",
    "        driver.quit()\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        ScrollCount = 0\n",
    "        results = driver.find_elements_by_xpath('//div[@class=\"FiltersInstantSearch\"]//div[@class=\"feed-item\"]')\n",
    "    except Exception as e:\n",
    "        driver.quit()\n",
    "        print(e)\n",
    "        continue\n",
    "        \n",
    "    if len(results) < 25:\n",
    "        driver.quit()\n",
    "        continue\n",
    "        \n",
    "    try:\n",
    "        while (len(results) % 40 == 0) and (ScrollCount < 4):\n",
    "            driver.execute_script(\"window.scrollTo(0,document.body.scrollHeight)\")\n",
    "            ScrollCount += 1\n",
    "            results = driver.find_elements_by_xpath('//div[@class=\"FiltersInstantSearch\"]//div[@class=\"feed-item\"]')\n",
    "            time.sleep(1)\n",
    "    except Exception as e:\n",
    "        driver.quit()\n",
    "        print(e)\n",
    "        continue\n",
    "     \n",
    "    try:\n",
    "        for result in results:\n",
    "                link = result.find_element_by_xpath('./a').get_attribute(\"href\")\n",
    "                Link.append(link)\n",
    "\n",
    "    except Exception as e:\n",
    "        driver.quit()\n",
    "        print(e)\n",
    "        continue\n",
    "    \n",
    "    \n",
    "    driver.quit()\n",
    "    \n",
    "    count += 1\n",
    "    \n",
    "    if count%10 == 0:\n",
    "        print(count)\n",
    "    \n",
    "        \n",
    "# Turn the lists into a DataFrame \n",
    "ItemDF_5=pd.DataFrame(Link,columns=['Link'])\n",
    "ItemDF_5.to_csv('Sold_Items_5.csv')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "Link =[]\n",
    "for link in sold_links_6['Link']:\n",
    "    try:\n",
    "        # Set a URL\n",
    "        base_url = link\n",
    "\n",
    "        # open up chrome. \n",
    "        chrome_options = webdriver.ChromeOptions()\n",
    "        chrome_options.add_argument(\"--start-maximized\")\n",
    "        driver = webdriver.Chrome(\"C:/Users/alber/Documents/chromedriver.exe\",options=chrome_options)\n",
    "        driver.get(base_url)\n",
    "\n",
    "        # wait 30 sec\n",
    "        timeout = 20\n",
    "        \n",
    "    except Exception as e:\n",
    "        driver.quit()\n",
    "        print(e)\n",
    "        continue     \n",
    "    \n",
    "    try:\n",
    "        WebDriverWait(driver, timeout).until(EC.visibility_of_element_located((By.XPATH, \"//div[@class='feed-item']\")))\n",
    "    except TimeoutException:\n",
    "        print(\"Timed out waiting for page to load\")\n",
    "        driver.quit()\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        ScrollCount = 0\n",
    "        results = driver.find_elements_by_xpath('//div[@class=\"FiltersInstantSearch\"]//div[@class=\"feed-item\"]')\n",
    "    except Exception as e:\n",
    "        driver.quit()\n",
    "        print(e)\n",
    "        continue\n",
    "        \n",
    "    if len(results) < 25:\n",
    "        driver.quit()\n",
    "        continue\n",
    "        \n",
    "    try:\n",
    "        while (len(results) % 40 == 0) and (ScrollCount < 4):\n",
    "            driver.execute_script(\"window.scrollTo(0,document.body.scrollHeight)\")\n",
    "            ScrollCount += 1\n",
    "            results = driver.find_elements_by_xpath('//div[@class=\"FiltersInstantSearch\"]//div[@class=\"feed-item\"]')\n",
    "            time.sleep(1)\n",
    "    except Exception as e:\n",
    "        driver.quit()\n",
    "        print(e)\n",
    "        continue\n",
    "     \n",
    "    try:\n",
    "        for result in results:\n",
    "                link = result.find_element_by_xpath('./a').get_attribute(\"href\")\n",
    "                Link.append(link)\n",
    "\n",
    "    except Exception as e:\n",
    "        driver.quit()\n",
    "        print(e)\n",
    "        continue\n",
    "    \n",
    "    \n",
    "    driver.quit()\n",
    "    \n",
    "    count += 1\n",
    "    \n",
    "    if count%10 == 0:\n",
    "        print(count)\n",
    "    \n",
    "        \n",
    "# Turn the lists into a DataFrame\n",
    "ItemDF_6=pd.DataFrame(Link,columns=['Link'])\n",
    "ItemDF_6.to_csv('Sold_Items_6.csv')    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
